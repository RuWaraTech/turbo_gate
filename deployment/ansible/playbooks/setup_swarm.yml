- name: Setup Docker Swarm Cluster on Hetzner Cloud
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    swarm_advertise_addr: "{{ internal_ip }}"
    swarm_listen_addr: "{{ internal_ip }}:2377"
    join_retries: 3
    join_retry_delay: 10  # Increased from 5 to 10 seconds for better stability

  tasks:
    - name: Install Docker using official script
      ansible.builtin.get_url:
        url: https://get.docker.com
        dest: /tmp/get-docker.sh
        mode: '0755'
      changed_when: false

    - name: Execute Docker installation
      ansible.builtin.command: /tmp/get-docker.sh
      args:
        removes: /tmp/get-docker.sh

    - name: Ensure Docker service is running
      ansible.builtin.systemd:
        name: docker
        state: started
        enabled: yes
        daemon_reload: yes

    - name: Install required Python packages
      ansible.builtin.pip:
        name:
          - docker # The 'docker' Python SDK for the Docker modules
          - docker-compose # For docker-compose CLI, if you use it later (not strictly for swarm)
          - jsondiff # Not typically required for Docker modules, but kept if you need it elsewhere
        state: present

    - name: Configure non-root Docker access
      ansible.builtin.user:
        name: "{{ ansible_user | default('ubuntu') }}" # Assumes ubuntu as default if ansible_user is not defined
        groups: docker
        append: yes
      # To apply this, you might need to log out and back in,
      # or run 'newgrp docker' on the target host. For Ansible,
      # it usually applies on the next SSH connection.

    - name: Initialize Docker Swarm on manager
      community.docker.docker_swarm:
        state: present
        advertise_addr: "{{ swarm_advertise_addr }}"
        listen_addr: "{{ swarm_listen_addr }}"
        force: yes # Use with caution: forces recreation if swarm already exists
      when: "'manager' in group_names"
      register: swarm_init
      retries: 3
      delay: 5

    - name: Store swarm tokens securely
      ansible.builtin.set_fact:
        docker_worker_token: "{{ swarm_init.swarm_facts.JoinTokens.Worker }}"
        docker_manager_token: "{{ swarm_init.swarm_facts.JoinTokens.Manager }}"
      when:
        - "'manager' in group_names"
        - swarm_init.swarm_facts is defined
        - swarm_init.changed # Only set if swarm_init actually changed something (i.e., created a new swarm)
      no_log: true # Prevents token exposure in logs

    # Manager readiness check for workers to ensure manager's swarm port is open
    - name: Verify manager readiness for workers to join
      ansible.builtin.wait_for:
        host: "{{ hostvars[groups['manager'][0]]['internal_ip'] }}" # Target the manager's IP
        port: 2377
        state: started
        delay: 5
        timeout: 120
      when: "'workers' in group_names" # This task runs on workers, waiting for the manager
      delegate_to: localhost # Run from control node if you want to ensure it's connecting to manager

    - name: Join workers to swarm
      community.docker.docker_swarm:
        state: join
        advertise_addr: "{{ swarm_advertise_addr }}"
        join_token: "{{ hostvars[groups['manager'][0]]['docker_worker_token'] }}" # Correctly access token from manager's hostvars
        remote_addrs:
          - "{{ hostvars[groups['manager'][0]]['internal_ip'] }}:2377"
      when:
        - "'workers' in group_names"
        - hostvars[groups['manager'][0]]['docker_worker_token'] is defined # Ensure token is available
      retries: "{{ join_retries }}"
      delay: "{{ join_retry_delay }}"
      register: join_result
      until: join_result is succeeded
      # Adding a 'changed_when: false' if you only care about successful join
      # and not when the worker is already part of swarm and state is 'present'
      # changed_when: false # Uncomment if you don't want this task to report 'changed' on subsequent runs




    - name: Validate swarm status on manager node
      block:
        - name: Get Docker Swarm cluster-level facts
          community.docker.docker_swarm_info:
          register: cluster_info
          when: "'manager' in group_names"

        - name: Get Docker Swarm node-level facts (for all nodes)
          community.docker.docker_swarm_info:
            nodes: true # Crucial: Request node-specific information
          register: node_facts
          when: "'manager' in group_names"

        - name: Debug 'cluster_info' (overall swarm details)
          ansible.builtin.debug:
            msg: "{{ cluster_info | to_nice_json }}"
          when: "'manager' in group_names"

        - name: Debug 'node_facts' (specific node states)
          ansible.builtin.debug:
            msg: "{{ node_facts | to_nice_json }}"
          when: "'manager' in group_names"

        - name: Set fact for the local manager node's details
          ansible.builtin.set_fact:
            # CORRECTED LINE HERE: 'Description.Hostname' changed to just 'Hostname'
            local_manager_node: "{{ node_facts.nodes | selectattr('Hostname', 'equalto', ansible_hostname) | first }}"
          when:
            - "'manager' in group_names"
            - node_facts.nodes is defined
            - node_facts.nodes | length > 0 # Ensure there are nodes to select from

        - name: Assert local manager node is healthy and leader
          ansible.builtin.assert:
            that:
              # Node Status: 'ready' is the healthy state for a node in the swarm.
              - local_manager_node.Status.State == "ready"
              # Manager Status: Your debug shows 'ManagerStatus: "Leader"' for the manager.
              # So, we check if ManagerStatus is defined and if it contains "Leader".
              - local_manager_node.ManagerStatus is defined
              - local_manager_node.ManagerStatus == "Leader"
              # Also ensure Reachability is "reachable", if it were a direct key, but it's not present in your debug for manager.
              # The 'Leader' string itself is the indicator.
            fail_msg: "Swarm manager node '{{ ansible_hostname }}' is not in a healthy state or is not the leader. Current state: {{ local_manager_node.Status.State | default('N/A') }}, Manager Status: {{ local_manager_node.ManagerStatus | default('N/A') }}"
            success_msg: "Swarm manager node '{{ ansible_hostname }}' is active and healthy."
          when: "'manager' in group_names and local_manager_node is defined"

      when: "'manager' in group_names" # This 'when' applies to the entire validation block


    - name: Create overlay network
      community.docker.docker_network:
        name: turbogate_network
        driver: overlay
        attachable: yes
        driver_options:
          encrypted: "true"
      when: "'manager' in group_names"

    - name: Display swarm connection info
      ansible.builtin.debug:
        msg: |
          SWARM DEPLOYMENT SUCCESSFUL
          ===========================
          Manager Node: {{ groups['manager'][0] }}
          Internal IP: {{ hostvars[groups['manager'][0]]['internal_ip'] }}
          Swarm Token: (secured - check manager node for actual value via `docker swarm join-token worker`)
          Overlay Network: turbogate_network
          Node Count: {{ groups['workers']|length + 1 }}
      when: "'manager' in group_names"
      run_once: true